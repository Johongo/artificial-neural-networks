{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('3.8.5': pyenv)",
   "display_name": "Python 3.8.5 64-bit ('3.8.5': pyenv)",
   "metadata": {
    "interpreter": {
     "hash": "fbf599a900ce400eb7a3a7d921c6d7bb2d1082ff9f9e1eebf399936d666fe1df"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(10000, 3)\n"
    }
   ],
   "source": [
    "# Data\n",
    "training = np.genfromtxt('training_set.csv', delimiter = ',')\n",
    "validation = np.genfromtxt('validation_set.csv', delimiter = ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0: 0.1516\nEpoch 1: 0.1516\nEpoch 2: 0.1516\nEpoch 3: 0.1364\nEpoch 4: 0.1366\nEpoch 5: 0.1348\nEpoch 6: 0.1366\nEpoch 7: 0.1326\nEpoch 8: 0.1346\nEpoch 9: 0.131\nEpoch 10: 0.1274\nEpoch 11: 0.1324\nEpoch 12: 0.1292\nEpoch 13: 0.1262\nEpoch 14: 0.1262\nEpoch 15: 0.1214\nEpoch 16: 0.1262\nEpoch 17: 0.1252\nEpoch 18: 0.1226\nEpoch 19: 0.1226\nEpoch 20: 0.125\nEpoch 21: 0.1334\nEpoch 22: 0.1208\nEpoch 23: 0.1242\nEpoch 24: 0.1266\nEpoch 25: 0.1242\nEpoch 26: 0.122\nEpoch 27: 0.1226\nEpoch 28: 0.1204\nEpoch 29: 0.1194\nConverged after 29 iterations...\nw1: [[-1.66246179 -2.63131697]\n [ 6.09365405  0.64582226]\n [ 1.52910986  3.12039609]\n [ 3.09183992  0.2790167 ]\n [ 2.51355266 -0.65955321]\n [-4.41854119  4.65764226]\n [-1.51286511  1.57901761]\n [ 2.0221333   6.78273928]]\nw2: [[ 0.12298495 -0.7820567  -0.60282765  0.54845405  1.63406847  0.95654461\n   0.38979667  0.6601195 ]\n [ 0.35742738  1.95164394 -0.68943284 -1.44686468  0.73545639  2.75914139\n   2.45752393  3.37214771]\n [-1.72354926  1.19807265  0.85418766  2.69958884  2.52539188 -2.57329395\n   0.78409088 -2.91641968]\n [ 1.57236103 -2.17455183 -2.81938187 -1.20357475  0.52682943  2.4323744\n   0.40473756 -2.48410071]]\nw3: [[ 1.78154964 -1.11905171 -1.10799624 -1.17507401]]\nt1: [[ 1.29403712]\n [-0.13395335]\n [-1.67278517]\n [ 1.41046959]\n [ 1.83639525]\n [-0.50679236]\n [ 1.2753257 ]\n [ 0.65663905]]\nt2: [[1.41699084]\n [1.77835633]\n [0.78387732]\n [1.06663859]]\nt3: [[0.4800462]]\n"
    }
   ],
   "source": [
    "m1 = 8\n",
    "m2 = 4\n",
    "\n",
    "class TwoLayerPerceptron():\n",
    "    def __init__(self, training, validation, learning_rate = 0.02, epochs = 1000):\n",
    "        self.training = training\n",
    "        self.validation = validation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.w1 = self._initialize_weights(m1, 2)\n",
    "        self.w2 = self._initialize_weights(m2, m1)\n",
    "        self.w3 = self._initialize_weights(1, m2)\n",
    "        self.t1 = self._initialize_thresholds(m1, 1)\n",
    "        self.t2 = self._initialize_thresholds(m2, 1)\n",
    "        self.t3 = self._initialize_thresholds(1, 1)\n",
    "\n",
    "    def _initialize_weights(self, m, n, mu = 0, sigma = 1):\n",
    "        size = (m, n)\n",
    "        w = np.random.normal(mu, sigma, size = size)\n",
    "        return w\n",
    "\n",
    "    def _initialize_thresholds(self, m, n):\n",
    "        size = (m, n)\n",
    "        t = np.zeros(size)\n",
    "        return t\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        output1 = np.tanh(self.w1 @ np.transpose(inputs) - self.t1)\n",
    "        output2 = np.tanh(self.w2 @ output1 - self.t2)\n",
    "        output3 = np.tanh(self.w3 @ output2 - self.t3)\n",
    "        return output1, output2, output3\n",
    "\n",
    "    def propagate_back(self, inputs, target, output1, output2, output3):\n",
    "        error3 = (target - output3) * (1 - output3 ** 2)\n",
    "        error2 = np.multiply((np.transpose(self.w3) @ error3), (1 - output2 ** 2))\n",
    "        error1 = np.multiply((np.transpose(self.w2) @ error2), (1 - output1 ** 2))\n",
    "        dw3 = -1 * self.learning_rate * (-1 * error3 * np.transpose(output2))\n",
    "        dw2 = -1 * self.learning_rate * np.multiply(-1 * error2, np.transpose(output1))\n",
    "        dw1 = -1 * self.learning_rate * np.multiply(-1 * error1, inputs)\n",
    "        dt3 = -1 * self.learning_rate * error3\n",
    "        dt2 = -1 * self.learning_rate * error2\n",
    "        dt1 = -1 * self.learning_rate * error1\n",
    "        return dw1, dw2, dw3, dt1, dt2, dt3\n",
    "\n",
    "    def _update(self):\n",
    "        training = np.array(self.training.copy())\n",
    "        np.random.shuffle(training)\n",
    "        for pattern in training:\n",
    "            inputs = pattern[:-1].reshape(1,2)\n",
    "            target = pattern[-1].reshape(1,1)\n",
    "            output1, output2, output3 = self.feed_forward(inputs)\n",
    "            dw1, dw2, dw3, dt1, dt2, dt3 = self.propagate_back(inputs, target, output1, output2, output3)\n",
    "            self.w1 += dw1\n",
    "            self.w2 += dw2\n",
    "            self.w3 += dw3\n",
    "            self.t1 += dt1\n",
    "            self.t2 += dt2\n",
    "            self.t3 += dt3       \n",
    "\n",
    "    def _classification_error(self):\n",
    "        validation = np.array(self.validation.copy())\n",
    "        length = validation.shape[0]\n",
    "        inputs = validation[:,:-1].reshape(length, 2)\n",
    "        targets = validation[:,-1].reshape(length, 1)\n",
    "        output3 = self.feed_forward(inputs)[-1]\n",
    "        errors = np.sum(np.abs(np.sign(output3) - np.transpose(targets)))\n",
    "        return 0.5 * errors / length\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self._update()\n",
    "            error = self._classification_error()\n",
    "            print(f\"Epoch {epoch}: {error}\")\n",
    "            if error < 0.12:\n",
    "                print(f\"Converged after {epoch} iterations...\")\n",
    "                return True\n",
    "        print(\"\\nNo convergence...\\n\")\n",
    "        return False\n",
    "\n",
    "    def print(self):\n",
    "        print(f\"w1: {self.w1}\")\n",
    "        print(f\"w2: {self.w2}\")\n",
    "        print(f\"w3: {self.w3}\")\n",
    "        print(f\"t1: {self.t1}\")\n",
    "        print(f\"t2: {self.t2}\")\n",
    "        print(f\"t3: {self.t3}\")\n",
    "\n",
    "def main():\n",
    "    network = TwoLayerPerceptron(training, validation)\n",
    "    network.train()\n",
    "    np.savetxt(\"w1.csv\", network.w1, delimiter=\",\")\n",
    "    np.savetxt(\"w2.csv\", network.w2, delimiter=\",\")\n",
    "    np.savetxt(\"w3.csv\", network.w3, delimiter=\",\")\n",
    "    np.savetxt(\"t1.csv\", network.t1, delimiter=\",\")\n",
    "    np.savetxt(\"t2.csv\", network.t2, delimiter=\",\")\n",
    "    np.savetxt(\"t3.csv\", network.t3, delimiter=\",\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ]
}